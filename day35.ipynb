{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4f9b7-f7d3-494a-980a-6b052cf4f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-to-One (1-to-1) – Sentiment Classification for a Single Word\n",
    "#Classify the sentiment of a single word embedding (Positive / Negative)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define One-to-One model\n",
    "class OneToOneModel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(OneToOneModel, self)._init_()\n",
    "        self.fc = nn.Linear(100, 2)  # 100 input features → 2 classes (positive, negative)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Simulate a random word vector of 100 dimensions\n",
    "x = torch.randn(1, 100)\n",
    "\n",
    "# Create the model\n",
    "model = OneToOneModel()\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = output.argmax().item()\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8425ec7-cabe-41a2-b251-e5d4cf2ad837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-to-Many (1-to-Many) – Image Captioning\n",
    "#Task: Generate a sequence (caption) from a single image\n",
    "#This uses:\n",
    "\n",
    "#CNN (ResNet18) as encoder\n",
    "\n",
    "#LSTM as decoder\n",
    "\n",
    "#Code: CNN + LSTM for Captioning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pretrained ResNet18 model and remove the final fully connected layer\n",
    "cnn = models.resnet18(pretrained=True)\n",
    "modules = list(cnn.children())[:-1]  # Remove final FC layer\n",
    "cnn = nn.Sequential(*modules)\n",
    "\n",
    "# Freeze CNN parameters to avoid training them\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Decoder for Captioning\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def _init_(self, embed_size, hidden_size, vocab_size):\n",
    "        super(CaptionDecoder, self)._init_()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # captions shape: [batch_size, caption_len]\n",
    "        embeddings = self.embed(captions)  # [batch_size, caption_len, embed_size]\n",
    "        features = features.unsqueeze(1)   # [batch_size, 1, embed_size]\n",
    "        inputs = torch.cat((features, embeddings), dim=1)  # Concatenate image feature as first input\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n",
    "\n",
    "# Dummy inputs for demo purposes\n",
    "vocab_size = 5000\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "decoder = CaptionDecoder(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Simulate an image (batch of 1 image)\n",
    "image = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Extract features using CNN encoder\n",
    "with torch.no_grad():\n",
    "    features = cnn(image)  # Output shape: [1, 512, 1, 1]\n",
    "    features = features.view(features.size(0), -1)  # Flatten: [1, 512]\n",
    "\n",
    "# Project image features to embedding size\n",
    "feature_projector = nn.Linear(512, embed_size)\n",
    "projected_features = feature_projector(features)  # [1, 256]\n",
    "\n",
    "# Simulated input caption (token IDs)\n",
    "caption_input = torch.randint(0, vocab_size, (1, 10))  # batch_size=1, length=10\n",
    "\n",
    "# Generate outputs (logits)\n",
    "outputs = decoder(projected_features, caption_input)\n",
    "print(\"Output shape (logits):\", outputs.shape)  # [1, 11, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935b99f-1d01-48b8-a7c2-ec7c773bd7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Many-to-One: Sentiment Classification (IMDB-style)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ManyToOneLSTM(nn.Module):\n",
    "    def _init_(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super()._init_()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                       # (batch_size, seq_len, embed_dim)\n",
    "        _, (hidden, _) = self.lstm(x)              # hidden: (1, batch_size, hidden_dim)\n",
    "        return self.fc(hidden[-1])                 # (batch_size, output_dim)\n",
    "\n",
    "# Example\n",
    "vocab_size = 5000\n",
    "model = ManyToOneLSTM(vocab_size, 128, 256, 2)     # Binary classification\n",
    "x = torch.randint(0, vocab_size, (4, 20))          # batch_size=4, seq_len=20\n",
    "out = model(x)\n",
    "print(\"Output shape:\", out.shape)                 # (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bff96-f393-44b2-ade1-f839678eb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Aligned Many-to-Many: POS Tagging\n",
    "class AlignedManyToManyLSTM(nn.Module):\n",
    "    def _init_(self, vocab_size, embed_dim, hidden_dim, tag_size):\n",
    "        super()._init_()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)                # (batch, seq_len, hidden_dim)\n",
    "        return self.fc(lstm_out)                  # (batch, seq_len, tag_size)\n",
    "\n",
    "# Example\n",
    "vocab_size = 5000\n",
    "tag_size = 10\n",
    "model = AlignedManyToManyLSTM(vocab_size, 128, 256, tag_size)\n",
    "x = torch.randint(0, vocab_size, (2, 15))         # batch of 2 sequences of length 15\n",
    "out = model(x)\n",
    "print(\"Output shape:\", out.shape)                # (2, 15, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
