{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60da15b-9af7-4d46-8c33-1b9ea89b53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokanization\n",
    "1.word tokanization\n",
    "2.sentence tokanization\n",
    "\n",
    "#segmentation\n",
    "making space in words\n",
    "\n",
    "#stemming\n",
    "removing suffix and prefix from words and converting words in base form\n",
    "\n",
    "#text normalization\n",
    "cleaning data before processing\n",
    "\n",
    "#regular expressions\n",
    "used for matching , serching and replacing words or finding phone numbers\n",
    "\n",
    "#parts of speach tagging\n",
    "tagging word as noun , verb or object\n",
    "\n",
    "#name intity recognition\n",
    "finding name in sentences\n",
    "\n",
    "#chunking\n",
    "converting text in meaningful phrase\n",
    "\n",
    "#lamatization\n",
    "reducing words in to simple forms\n",
    "\n",
    "#import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download WordNet data (run once)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "word = \"happy\"\n",
    "\n",
    "# Get synonyms for the word\n",
    "synonyms = set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "\n",
    "print(\"Synonyms of\", word, \":\")\n",
    "print(synonyms)\n",
    "\n",
    "finding meaning of words\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f8d50-ac54-4d50-bbb4-7e16f008c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text to tokenize\n",
    "text = \"Natural Language Processing is amazing! Let's master it together.\"\n",
    "\n",
    "# Tokenize into words\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\")\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a990c60-45db-4ba8-9686-5f6827f80839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Longer text\n",
    "text = \"Natural Language Processing is amazing! It helps machines understand human language. Let's master NLP together.\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokens:\")\n",
    "print(sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a84219-3d0f-4ea3-ae91-8b0153931ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny dictionary of known words\n",
    "dictionary = {\"i\", \"am\", \"learning\", \"nlp\", \"natural\", \"language\", \"processing\", \"is\", \"fun\"}\n",
    "\n",
    "def word_segment(text):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        for j in range(i+1, len(text)+1):\n",
    "            word = text[i:j]\n",
    "            if word in dictionary:\n",
    "                result.append(word)\n",
    "                i = j - 1\n",
    "                break\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "text = \"iamlearningnlp\"\n",
    "tokens = word_segment(text)\n",
    "\n",
    "print(\"Segmented Words:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce82d64-da73-44f9-8899-f60370be6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"happiness\", \"playing\", \"easily\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04813331-a81c-4dbf-b246-b764813411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"I can't believe it's 5 o'clock! Apples are great, and so are oranges.\"\n",
    "# Lowercase\n",
    "text = text.lower()\n",
    "# Expand contractions (manual for this example)\n",
    "text = text.replace(\"can't\", \"cannot\").replace(\"it's\", \"it is\").replace(\"o'clock\", \"oclock\")\n",
    "# Remove punctuation\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e9056-549f-454c-b09d-c8fd3f5a764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "text = \"Hello! This string has #hashtags, @mentions, and $ymbol$.\"\n",
    "print(remove_special_chars(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304b697-890e-456a-9ab0-257a3d010083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821e5bb-6c68-4dcd-91eb-ac599e9e4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model with NER capabilities\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion in 2024.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print detected entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378e8e8-7969-4ddd-b297-e13b64d6b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tags:\")\n",
    "print(pos_tags)\n",
    "\n",
    "# Define a chunk grammar, e.g., for Noun Phrases (NP)\n",
    "# NP chunk: determiner (DT) + adjectives (JJ) 0 or more + noun (NN or NNS)\n",
    "chunk_grammar = r\"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN|NNS>}   # Chunk determiner/adjectives + noun(s)\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "# Parse the POS-tagged sentence to get chunks\n",
    "chunked = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(\"\\nChunked Tree:\")\n",
    "print(chunked)\n",
    "\n",
    "# To visualize the chunk tree (optional, requires GUI)\n",
    "# chunked.draw()\n",
    "\n",
    "# Extract and print noun phrase chunks\n",
    "print(\"\\nNoun Phrase Chunks:\")\n",
    "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "    np = \" \".join(word for word, tag in subtree.leaves())\n",
    "    print(np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfefc8-9802-404c-9b84-d63efc4ac748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"ran\", \"runs\", \"better\", \"octopi\"]\n",
    "\n",
    "# Lemmatize each word (default is noun)\n",
    "for word in words:\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word)}\")\n",
    "\n",
    "# Lemmatize with part of speech (e.g., verb)\n",
    "print(\"running (verb) -->\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "print(\"better (adjective) -->\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65211d6c-854f-4ce0-8d4f-f7b1f829a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download WordNet data (run once)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "word = \"happy\"\n",
    "\n",
    "# Get synonyms for the word\n",
    "synonyms = set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "\n",
    "print(\"Synonyms of\", word, \":\")\n",
    "print(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7fc3c6-43cb-4fd2-8bc0-252129e35ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1\n",
    "#change paragraph into sentences and then into words\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraph = \"Good morning Dr. Adams. The patient is waiting for you in room number 3.\"\n",
    "\n",
    "# Split paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Split each sentence into words and print\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa9bcf-3e1f-4edf-97cd-1c54006c393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_break(s, word_dict):\n",
    "    result = []\n",
    "    def backtrack(start, path):\n",
    "        if start == len(s):\n",
    "            result.append(path[:])\n",
    "            return\n",
    "        for end in range(start + 1, len(s) + 1):\n",
    "            word = s[start:end]\n",
    "            if word in word_dict:\n",
    "                path.append(word)\n",
    "                backtrack(end, path)\n",
    "                path.pop()\n",
    "    backtrack(0, [])\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "s = \"thisisatask\"\n",
    "word_dict = {\"this\", \"is\", \"a\", \"task\"}\n",
    "all_splits = word_break(s, word_dict)\n",
    "# If you want just one valid split:\n",
    "if all_splits:\n",
    "    print(all_splits[0])\n",
    "else:\n",
    "    print(\"No valid split found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710469a-56bf-49ae-aaf3-e8f804f41a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_break(s, word_dict):\n",
    "    result = []\n",
    "    def backtrack(start, path):\n",
    "        if start == len(s):\n",
    "            result.append(path[:])\n",
    "            return\n",
    "        for end in range(start + 1, len(s) + 1):\n",
    "            word = s[start:end]\n",
    "            if word in word_dict:\n",
    "                path.append(word)\n",
    "                backtrack(end, path)\n",
    "                path.pop()\n",
    "    backtrack(0, [])\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "s = \"thisisatask\"\n",
    "word_dict = {\"this\", \"is\", \"a\", \"task\"}\n",
    "all_splits = word_break(s, word_dict)\n",
    "# If you want just one valid split:\n",
    "if all_splits:\n",
    "    print(all_splits[0])\n",
    "else:\n",
    "    print(\"No valid split found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ef350-7550-4064-ae37-ba4f1fe2dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_and_correct_text(text):\n",
    "    # Dictionary of common misspellings and their corrections\n",
    "    corrections = {\n",
    "        \"teh\": \"the\",\n",
    "        \"recieve\": \"receive\",\n",
    "        \"adress\": \"address\",\n",
    "        \"occured\": \"occurred\",\n",
    "        \"seperate\": \"separate\",\n",
    "        \"definately\": \"definitely\",\n",
    "        \"wich\": \"which\",\n",
    "        \"thier\": \"their\",\n",
    "        \"acommodate\": \"accommodate\",\n",
    "        \"becuase\": \"because\",\n",
    "        \"untill\": \"until\",\n",
    "        \"goverment\": \"government\",\n",
    "        \"adress\": \"address\",\n",
    "        \"wich\": \"which\",\n",
    "        \"wierd\": \"weird\",\n",
    "        \"beleive\": \"believe\",\n",
    "        \"enviroment\": \"environment\",\n",
    "        \"reciept\": \"receipt\"\n",
    "    }\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Correct common spelling mistakes word by word\n",
    "    words = text.split()\n",
    "    corrected_words = [corrections.get(word, word) for word in words]\n",
    "    \n",
    "    # Join back into a string\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \"Teh goverment has definately adresssed teh enviroment issues, untill now.\"\n",
    "cleaned_text = clean_and_correct_text(sample_text)\n",
    "print(cleaned_text)\n",
    "# Output: \"the government has definitely addressed the environment issues until now\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f548e5-f493-4b3b-b65d-57591879f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task\n",
    "#build a text cleaning and normalization pipeline,,, design a full text preprocessing pipelinethat can take any raw text,,,,remove noice ,,,,tokinazitation,,,text normalization ,,, stemming and lemilation,,,pos tagging\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags and URLs\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_pipeline(raw_text):\n",
    "    # 1. Clean and normalize text\n",
    "    text = clean_text(raw_text)\n",
    "    # 2. Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 3. Word tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # 4. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_nostop = [word for word in tokens if word not in stop_words]\n",
    "    # 5. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in tokens_nostop]\n",
    "    # 6. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens_nostop]\n",
    "    # 7. POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens_nostop)\n",
    "    return {\n",
    "        'cleaned_text': text,\n",
    "        'sentences': sentences,\n",
    "        'tokens': tokens,\n",
    "        'tokens_no_stopwords': tokens_nostop,\n",
    "        'stemmed': stemmed,\n",
    "        'lemmatized': lemmatized,\n",
    "        'pos_tags': pos_tags\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"The quick brown foxes jumped over the lazy dogs! Visit http://example.com for more info.\"\n",
    "result = preprocess_pipeline(raw_text)\n",
    "print(\"Cleaned Text:\", result['cleaned_text'])\n",
    "print(\"Sentences:\", result['sentences'])\n",
    "print(\"Tokens:\", result['tokens'])\n",
    "print(\"Tokens (no stopwords):\", result['tokens_no_stopwords'])\n",
    "print(\"Stemmed:\", result['stemmed'])\n",
    "print(\"Lemmatized:\", result['lemmatized'])\n",
    "print(\"POS Tags:\", result['pos_tags'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
