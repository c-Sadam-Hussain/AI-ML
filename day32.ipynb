{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f56597-11db-4bd0-b6e9-a21210b5a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recurrent Neural Networks (RNNs)\n",
    "\n",
    "#drawbacks\n",
    "vanishing gradient\n",
    "exploring gradient \n",
    "\n",
    "we use LSTM (long short term memory)\n",
    "it use gating mechanism input gate , forget gate , output gate \n",
    "\n",
    "\n",
    "GRU algorithm ()\n",
    "update gate \n",
    "reset gate\n",
    "condidate activation (it combine current state with reset state)\n",
    "final hidden state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b37e629-80dd-4790-824d-c52c10f197c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Load and preprocess\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Encode text\n",
    "def encode(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Define collate function\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label == 'pos' else 0)\n",
    "        texts.append(torch.tensor(encode(text), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels), texts\n",
    "\n",
    "# Model\n",
    "class GRUModel(nn.Module):\n",
    "    def _init_(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self)._init_()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "# Training\n",
    "def train_model():\n",
    "    model = GRUModel(len(vocab), 64, 128, 1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_iter = IMDB(split='train')\n",
    "    dataloader = DataLoader(list(train_iter), batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for labels, texts in dataloader:\n",
    "            preds = model(texts).squeeze()\n",
    "            loss = criterion(preds, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51c1684-d0e1-4982-bbfe-7dcacec9cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp312-cp312-win_amd64.whl (215.1 MB)\n",
      "   ---------------------------------------- 0.0/215.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/215.1 MB 2.8 MB/s eta 0:01:17\n",
      "   ---------------------------------------- 1.0/215.1 MB 3.1 MB/s eta 0:01:09\n",
      "   ---------------------------------------- 1.6/215.1 MB 2.9 MB/s eta 0:01:14\n",
      "   ---------------------------------------- 2.4/215.1 MB 2.8 MB/s eta 0:01:17\n",
      "    --------------------------------------- 2.9/215.1 MB 2.8 MB/s eta 0:01:18\n",
      "    --------------------------------------- 3.4/215.1 MB 2.7 MB/s eta 0:01:18\n",
      "    --------------------------------------- 3.7/215.1 MB 2.6 MB/s eta 0:01:22\n",
      "    --------------------------------------- 4.5/215.1 MB 2.6 MB/s eta 0:01:21\n",
      "    --------------------------------------- 4.5/215.1 MB 2.6 MB/s eta 0:01:21\n",
      "    --------------------------------------- 5.0/215.1 MB 2.3 MB/s eta 0:01:31\n",
      "   - -------------------------------------- 5.5/215.1 MB 2.3 MB/s eta 0:01:30\n",
      "   - -------------------------------------- 5.5/215.1 MB 2.3 MB/s eta 0:01:30\n",
      "   - -------------------------------------- 6.0/215.1 MB 2.2 MB/s eta 0:01:36\n",
      "   - -------------------------------------- 7.1/215.1 MB 2.3 MB/s eta 0:01:29\n",
      "   - -------------------------------------- 7.6/215.1 MB 2.4 MB/s eta 0:01:28\n",
      "   - -------------------------------------- 8.1/215.1 MB 2.4 MB/s eta 0:01:28\n",
      "   - -------------------------------------- 8.9/215.1 MB 2.4 MB/s eta 0:01:25\n",
      "   - -------------------------------------- 9.7/215.1 MB 2.5 MB/s eta 0:01:23\n",
      "   - -------------------------------------- 10.2/215.1 MB 2.5 MB/s eta 0:01:22\n",
      "   -- ------------------------------------- 11.0/215.1 MB 2.6 MB/s eta 0:01:20\n",
      "   -- ------------------------------------- 11.3/215.1 MB 2.5 MB/s eta 0:01:23\n",
      "   -- ------------------------------------- 12.1/215.1 MB 2.6 MB/s eta 0:01:20\n",
      "   -- ------------------------------------- 12.6/215.1 MB 2.5 MB/s eta 0:01:21\n",
      "   -- ------------------------------------- 13.1/215.1 MB 2.5 MB/s eta 0:01:20\n",
      "   -- ------------------------------------- 13.9/215.1 MB 2.6 MB/s eta 0:01:19\n",
      "   -- ------------------------------------- 14.4/215.1 MB 2.6 MB/s eta 0:01:18\n",
      "   -- ------------------------------------- 15.2/215.1 MB 2.6 MB/s eta 0:01:17\n",
      "   -- ------------------------------------- 16.0/215.1 MB 2.6 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 16.8/215.1 MB 2.7 MB/s eta 0:01:15\n",
      "   --- ------------------------------------ 17.6/215.1 MB 2.7 MB/s eta 0:01:13\n",
      "   --- ------------------------------------ 18.1/215.1 MB 2.7 MB/s eta 0:01:13\n",
      "   --- ------------------------------------ 18.9/215.1 MB 2.7 MB/s eta 0:01:13\n",
      "   --- ------------------------------------ 19.1/215.1 MB 2.7 MB/s eta 0:01:13\n",
      "   --- ------------------------------------ 19.9/215.1 MB 2.7 MB/s eta 0:01:12\n",
      "   --- ------------------------------------ 20.4/215.1 MB 2.7 MB/s eta 0:01:11\n",
      "   --- ------------------------------------ 21.0/215.1 MB 2.7 MB/s eta 0:01:12\n",
      "   --- ------------------------------------ 21.5/215.1 MB 2.7 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 22.0/215.1 MB 2.7 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 22.3/215.1 MB 2.7 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 22.5/215.1 MB 2.7 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 23.1/215.1 MB 2.6 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 23.3/215.1 MB 2.6 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 23.9/215.1 MB 2.6 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 24.4/215.1 MB 2.6 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 24.9/215.1 MB 2.6 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 25.2/215.1 MB 2.5 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 25.7/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 26.0/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 26.5/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 27.0/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 27.3/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 27.8/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 28.3/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 28.8/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 29.4/215.1 MB 2.5 MB/s eta 0:01:15\n",
      "   ----- ---------------------------------- 29.9/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 30.1/215.1 MB 2.5 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 30.4/215.1 MB 2.4 MB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 30.4/215.1 MB 2.4 MB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 30.9/215.1 MB 2.4 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 31.5/215.1 MB 2.4 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 31.7/215.1 MB 2.4 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 31.7/215.1 MB 2.4 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 32.0/215.1 MB 2.3 MB/s eta 0:01:19\n",
      "   ----- ---------------------------------- 32.2/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 32.8/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 33.0/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 33.3/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 33.8/215.1 MB 2.3 MB/s eta 0:01:21\n",
      "   ------ --------------------------------- 34.3/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 34.9/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 35.1/215.1 MB 2.3 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 35.4/215.1 MB 2.2 MB/s eta 0:01:21\n",
      "   ------ --------------------------------- 35.9/215.1 MB 2.2 MB/s eta 0:01:21\n",
      "   ------ --------------------------------- 36.4/215.1 MB 2.2 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 36.7/215.1 MB 2.2 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 37.0/215.1 MB 2.2 MB/s eta 0:01:21\n",
      "   ------ --------------------------------- 37.2/215.1 MB 2.2 MB/s eta 0:01:22\n",
      "   ------ --------------------------------- 37.5/215.1 MB 2.2 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 38.0/215.1 MB 2.2 MB/s eta 0:01:21\n",
      "   ------- -------------------------------- 38.3/215.1 MB 2.2 MB/s eta 0:01:21\n",
      "   ------- -------------------------------- 38.5/215.1 MB 2.2 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 38.8/215.1 MB 2.2 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 39.3/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 39.8/215.1 MB 2.2 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 40.1/215.1 MB 2.2 MB/s eta 0:01:21\n",
      "   ------- -------------------------------- 40.6/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 40.9/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 41.4/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 41.9/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   ------- -------------------------------- 42.2/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   ------- -------------------------------- 42.5/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 43.0/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 43.3/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 43.8/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 43.8/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 44.3/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 44.8/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   -------- ------------------------------- 45.4/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 45.6/215.1 MB 2.1 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 45.9/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   -------- ------------------------------- 45.9/215.1 MB 2.1 MB/s eta 0:01:22\n",
      "   -------- ------------------------------- 46.1/215.1 MB 2.1 MB/s eta 0:01:23\n",
      "   -------- ------------------------------- 46.1/215.1 MB 2.1 MB/s eta 0:01:23\n",
      "   -------- ------------------------------- 46.4/215.1 MB 2.0 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 46.7/215.1 MB 2.0 MB/s eta 0:01:23\n",
      "   -------- ------------------------------- 46.9/215.1 MB 2.0 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 47.4/215.1 MB 2.0 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 48.2/215.1 MB 2.0 MB/s eta 0:01:23\n",
      "   -------- ------------------------------- 48.2/215.1 MB 2.0 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 48.8/215.1 MB 2.0 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 49.3/215.1 MB 2.0 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 49.8/215.1 MB 2.0 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 50.6/215.1 MB 2.0 MB/s eta 0:01:21\n",
      "   --------- ------------------------------ 51.1/215.1 MB 2.0 MB/s eta 0:01:21\n",
      "   --------- ------------------------------ 51.9/215.1 MB 2.1 MB/s eta 0:01:20\n",
      "   --------- ------------------------------ 52.7/215.1 MB 2.1 MB/s eta 0:01:19\n",
      "   --------- ------------------------------ 53.2/215.1 MB 2.1 MB/s eta 0:01:19\n",
      "   ---------- ----------------------------- 54.0/215.1 MB 2.1 MB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 54.5/215.1 MB 2.1 MB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 55.1/215.1 MB 2.1 MB/s eta 0:01:17\n",
      "   ---------- ----------------------------- 55.6/215.1 MB 2.1 MB/s eta 0:01:17\n",
      "   ---------- ----------------------------- 55.8/215.1 MB 2.1 MB/s eta 0:01:17\n",
      "   ---------- ----------------------------- 56.6/215.1 MB 2.1 MB/s eta 0:01:16\n",
      "   ---------- ----------------------------- 57.1/215.1 MB 2.1 MB/s eta 0:01:16\n",
      "   ---------- ----------------------------- 57.9/215.1 MB 2.1 MB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 58.7/215.1 MB 2.1 MB/s eta 0:01:14\n",
      "   ----------- ---------------------------- 59.5/215.1 MB 2.1 MB/s eta 0:01:13\n",
      "   ----------- ---------------------------- 60.3/215.1 MB 2.1 MB/s eta 0:01:13\n",
      "   ----------- ---------------------------- 61.1/215.1 MB 2.2 MB/s eta 0:01:12\n",
      "   ----------- ---------------------------- 62.1/215.1 MB 2.2 MB/s eta 0:01:11\n",
      "   ----------- ---------------------------- 62.9/215.1 MB 2.2 MB/s eta 0:01:10\n",
      "   ----------- ---------------------------- 63.7/215.1 MB 2.2 MB/s eta 0:01:09\n",
      "   ----------- ---------------------------- 64.2/215.1 MB 2.2 MB/s eta 0:01:09\n",
      "   ------------ --------------------------- 65.0/215.1 MB 2.2 MB/s eta 0:01:08\n",
      "   ------------ --------------------------- 65.8/215.1 MB 2.2 MB/s eta 0:01:08\n",
      "   ------------ --------------------------- 66.1/215.1 MB 2.2 MB/s eta 0:01:07\n",
      "   ------------ --------------------------- 66.8/215.1 MB 2.2 MB/s eta 0:01:07\n",
      "   ------------ --------------------------- 67.6/215.1 MB 2.2 MB/s eta 0:01:07\n",
      "   ------------ --------------------------- 68.2/215.1 MB 2.2 MB/s eta 0:01:06\n",
      "   ------------ --------------------------- 68.9/215.1 MB 2.2 MB/s eta 0:01:06\n",
      "   ------------ --------------------------- 69.7/215.1 MB 2.2 MB/s eta 0:01:05\n",
      "   ------------- -------------------------- 70.5/215.1 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------- -------------------------- 71.3/215.1 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------- -------------------------- 72.1/215.1 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------- -------------------------- 72.9/215.1 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------- -------------------------- 73.4/215.1 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------- -------------------------- 74.2/215.1 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------- -------------------------- 74.7/215.1 MB 2.3 MB/s eta 0:01:01\n",
      "   -------------- ------------------------- 75.5/215.1 MB 2.3 MB/s eta 0:01:00\n",
      "   -------------- ------------------------- 76.3/215.1 MB 2.3 MB/s eta 0:01:00\n",
      "   -------------- ------------------------- 77.1/215.1 MB 2.3 MB/s eta 0:01:00\n",
      "   -------------- ------------------------- 77.6/215.1 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------- ------------------------- 78.4/215.1 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------- ------------------------- 78.9/215.1 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------- ------------------------- 79.4/215.1 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------- ------------------------- 80.0/215.1 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------- ------------------------- 80.5/215.1 MB 2.3 MB/s eta 0:00:59\n",
      "   --------------- ------------------------ 81.0/215.1 MB 2.3 MB/s eta 0:00:58\n",
      "   --------------- ------------------------ 81.5/215.1 MB 2.3 MB/s eta 0:00:58\n",
      "   --------------- ------------------------ 82.1/215.1 MB 2.3 MB/s eta 0:00:58\n",
      "   --------------- ------------------------ 83.1/215.1 MB 2.3 MB/s eta 0:00:57\n",
      "   --------------- ------------------------ 83.6/215.1 MB 2.3 MB/s eta 0:00:57\n",
      "   --------------- ------------------------ 84.4/215.1 MB 2.3 MB/s eta 0:00:56\n",
      "   --------------- ------------------------ 85.2/215.1 MB 2.3 MB/s eta 0:00:56\n",
      "   --------------- ------------------------ 85.7/215.1 MB 2.3 MB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 86.2/215.1 MB 2.3 MB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 86.8/215.1 MB 2.3 MB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 87.3/215.1 MB 2.3 MB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 88.1/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 88.6/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 89.1/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 89.7/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 89.7/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 89.9/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 90.4/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 91.0/215.1 MB 2.3 MB/s eta 0:00:55\n",
      "   ----------------- ---------------------- 92.0/215.1 MB 2.3 MB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 93.1/215.1 MB 2.3 MB/s eta 0:00:53\n",
      "   ----------------- ---------------------- 93.8/215.1 MB 2.4 MB/s eta 0:00:52\n",
      "   ----------------- ---------------------- 94.9/215.1 MB 2.4 MB/s eta 0:00:51\n",
      "   ----------------- ---------------------- 95.7/215.1 MB 2.4 MB/s eta 0:00:51\n",
      "   ----------------- ---------------------- 96.5/215.1 MB 2.4 MB/s eta 0:00:50\n",
      "   ------------------ --------------------- 97.3/215.1 MB 2.4 MB/s eta 0:00:49\n",
      "   ------------------ --------------------- 97.8/215.1 MB 2.4 MB/s eta 0:00:49\n",
      "   ------------------ --------------------- 98.6/215.1 MB 2.4 MB/s eta 0:00:49\n",
      "   ------------------ --------------------- 99.1/215.1 MB 2.4 MB/s eta 0:00:48\n",
      "   ------------------ --------------------- 99.4/215.1 MB 2.4 MB/s eta 0:00:48\n",
      "   ------------------ --------------------- 100.1/215.1 MB 2.4 MB/s eta 0:00:48\n",
      "   ------------------ --------------------- 101.2/215.1 MB 2.4 MB/s eta 0:00:47\n",
      "   ------------------ --------------------- 102.0/215.1 MB 2.5 MB/s eta 0:00:46\n",
      "   ------------------- -------------------- 103.0/215.1 MB 2.5 MB/s eta 0:00:46\n",
      "   ------------------- -------------------- 103.8/215.1 MB 2.5 MB/s eta 0:00:45\n",
      "   ------------------- -------------------- 104.6/215.1 MB 2.5 MB/s eta 0:00:45\n",
      "   ------------------- -------------------- 105.4/215.1 MB 2.5 MB/s eta 0:00:44\n",
      "   ------------------- -------------------- 106.4/215.1 MB 2.5 MB/s eta 0:00:43\n",
      "   ------------------- -------------------- 107.2/215.1 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------- ------------------- 108.0/215.1 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------- ------------------- 108.8/215.1 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------- ------------------- 109.6/215.1 MB 2.6 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 110.1/215.1 MB 2.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 110.9/215.1 MB 2.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 111.7/215.1 MB 2.7 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 112.2/215.1 MB 2.7 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 113.0/215.1 MB 2.7 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 113.8/215.1 MB 2.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 114.6/215.1 MB 2.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 115.3/215.1 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 115.6/215.1 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 115.9/215.1 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 116.4/215.1 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 117.2/215.1 MB 2.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 117.7/215.1 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 118.8/215.1 MB 2.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 119.5/215.1 MB 2.8 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 120.1/215.1 MB 2.8 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 120.6/215.1 MB 2.8 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 121.4/215.1 MB 2.8 MB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 122.4/215.1 MB 2.8 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 122.9/215.1 MB 2.8 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 124.0/215.1 MB 2.8 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 124.8/215.1 MB 2.9 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 125.6/215.1 MB 2.9 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 126.4/215.1 MB 2.9 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 126.9/215.1 MB 2.9 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 127.1/215.1 MB 2.9 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 127.9/215.1 MB 2.9 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 128.5/215.1 MB 2.9 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 129.0/215.1 MB 2.9 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 129.5/215.1 MB 2.9 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 130.0/215.1 MB 2.9 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 130.8/215.1 MB 2.9 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 131.3/215.1 MB 2.9 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 132.1/215.1 MB 3.0 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 132.6/215.1 MB 3.0 MB/s eta 0:00:28\n",
      "   ------------------------ --------------- 133.2/215.1 MB 3.0 MB/s eta 0:00:28\n",
      "   ------------------------ --------------- 133.7/215.1 MB 3.0 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 134.5/215.1 MB 3.0 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 135.3/215.1 MB 3.0 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 135.8/215.1 MB 3.0 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 136.3/215.1 MB 3.0 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 136.8/215.1 MB 3.0 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 137.1/215.1 MB 3.1 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 137.6/215.1 MB 3.1 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 138.1/215.1 MB 3.1 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 138.7/215.1 MB 3.1 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 139.5/215.1 MB 3.1 MB/s eta 0:00:25\n",
      "   -------------------------- ------------- 140.2/215.1 MB 3.1 MB/s eta 0:00:25\n",
      "   -------------------------- ------------- 141.0/215.1 MB 3.1 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 142.1/215.1 MB 3.1 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 142.6/215.1 MB 3.1 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 143.9/215.1 MB 3.1 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 144.7/215.1 MB 3.1 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 145.2/215.1 MB 3.1 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 145.8/215.1 MB 3.1 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 146.3/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 146.8/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 147.3/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 147.6/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 148.1/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 148.4/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 148.6/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 148.9/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 149.4/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 149.7/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 149.9/215.1 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 150.5/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 150.7/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 151.3/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 151.5/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 151.8/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 152.0/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 152.6/215.1 MB 3.0 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 153.4/215.1 MB 3.0 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 153.6/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 154.1/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 154.7/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 154.9/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 155.5/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 155.7/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 156.5/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 156.8/215.1 MB 2.9 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 157.5/215.1 MB 2.9 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 158.3/215.1 MB 2.9 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 158.9/215.1 MB 2.9 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 159.6/215.1 MB 2.9 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 160.2/215.1 MB 2.9 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 161.0/215.1 MB 2.9 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 162.0/215.1 MB 2.9 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 162.8/215.1 MB 2.9 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 163.6/215.1 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 164.1/215.1 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 164.6/215.1 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 165.2/215.1 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 165.4/215.1 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 165.9/215.1 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 166.5/215.1 MB 2.9 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 167.2/215.1 MB 2.9 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 168.0/215.1 MB 2.9 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 168.6/215.1 MB 2.9 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 169.1/215.1 MB 2.9 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 169.9/215.1 MB 2.9 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 170.7/215.1 MB 2.9 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 171.2/215.1 MB 2.9 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 172.0/215.1 MB 2.9 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 172.5/215.1 MB 2.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 173.3/215.1 MB 2.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 174.3/215.1 MB 2.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 175.1/215.1 MB 2.9 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 175.6/215.1 MB 2.9 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 176.4/215.1 MB 2.9 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 177.2/215.1 MB 2.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 178.3/215.1 MB 3.0 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 178.8/215.1 MB 3.0 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 179.6/215.1 MB 3.0 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 180.4/215.1 MB 3.0 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 181.1/215.1 MB 3.0 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 181.9/215.1 MB 3.0 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 182.7/215.1 MB 3.0 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 183.2/215.1 MB 3.0 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 184.0/215.1 MB 3.0 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 184.5/215.1 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 185.1/215.1 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 185.6/215.1 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 186.4/215.1 MB 2.9 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 186.6/215.1 MB 2.9 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 187.4/215.1 MB 2.9 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 188.2/215.1 MB 2.9 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 188.7/215.1 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 189.3/215.1 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 190.1/215.1 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 190.8/215.1 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 191.9/215.1 MB 2.9 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 192.7/215.1 MB 2.9 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 193.7/215.1 MB 2.9 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 194.5/215.1 MB 2.9 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 195.3/215.1 MB 2.9 MB/s eta 0:00:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\RF\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\RF\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ------------------------------------ --- 195.8/215.1 MB 2.9 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 196.9/215.1 MB 2.9 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 197.7/215.1 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 198.2/215.1 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 198.7/215.1 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 199.2/215.1 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 199.8/215.1 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 200.3/215.1 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 200.8/215.1 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 201.3/215.1 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 202.1/215.1 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 202.6/215.1 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 203.4/215.1 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 203.9/215.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 204.5/215.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 205.0/215.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 205.5/215.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 206.3/215.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 207.1/215.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 207.9/215.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 208.4/215.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 209.5/215.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  210.2/215.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  211.3/215.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  212.1/215.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  212.9/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  213.4/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  213.9/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  214.4/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  214.7/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.0/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.0/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.0/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.0/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.0/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.0/215.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 215.1/215.1 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.0%2Bcpu-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.5 MB 907.1 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.6/2.5 MB 892.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 907.1 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 970.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.2 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.2/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "Successfully installed sympy-1.13.3 torch-2.7.0+cpu torchaudio-2.7.0+cpu torchvision-0.22.0+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fadf5e6c-05d8-49f9-bafc-5f3cf4274137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (0.22.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (2.7.0+cpu)\n",
      "Collecting torchtext\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchtext-0.18.0%2Bcpu-cp312-cp312-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 0.5/1.9 MB 764.3 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/1.9 MB 799.2 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/1.9 MB 799.2 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.0/1.9 MB 839.3 kB/s eta 0:00:02\n",
      "     -------------------------- ------------- 1.3/1.9 MB 860.9 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 932.2 kB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.8/1.9 MB 967.9 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 936.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.18.0+cpu\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Tokenizer and dataset\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Text encoding\n",
    "def encode(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label == 'pos' else 0)\n",
    "        texts.append(torch.tensor(encode(text), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels), texts\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    model = GRUModel(len(vocab), embed_dim=64, hidden_dim=128, output_dim=1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_iter = IMDB(split='train')\n",
    "    dataloader = DataLoader(list(train_iter), batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for labels, texts in dataloader:\n",
    "            preds = model(texts).squeeze()\n",
    "            loss = criterion(preds, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Run training\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e5e5b7-76d6-4285-bd8f-543e0f648446",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install PyTorch and TorchText if not already installed (for Colab or Jupyter)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cpu\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Install PyTorch and TorchText if not already installed (for Colab or Jupyter)\n",
    "# !pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Tokenizer and dataset\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Text encoding\n",
    "def encode(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label == 'pos' else 0)\n",
    "        texts.append(torch.tensor(encode(text), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels), texts\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    model = GRUModel(len(vocab), embed_dim=64, hidden_dim=128, output_dim=1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_iter = IMDB(split='train')\n",
    "    dataloader = DataLoader(list(train_iter), batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for labels, texts in dataloader:\n",
    "            preds = model(texts).squeeze()\n",
    "            loss = criterion(preds, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Run training\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff9e37a-ff51-4ba0-963a-cc7254469db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_ops.py:1392\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1387\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ctypes\\__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Load and preprocess\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Encode text\n",
    "def encode(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Define collate function\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label == 'pos' else 0)\n",
    "        texts.append(torch.tensor(encode(text), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels), texts\n",
    "\n",
    "# Model\n",
    "class GRUModel(nn.Module):\n",
    "    def _init_(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self)._init_()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "# Training\n",
    "def train_model():\n",
    "    model = GRUModel(len(vocab), 64, 128, 1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_iter = IMDB(split='train')\n",
    "    dataloader = DataLoader(list(train_iter), batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for labels, texts in dataloader:\n",
    "            preds = model(texts).squeeze()\n",
    "            loss = criterion(preds, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d58d69-50f6-4690-b370-381bfdbfd8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (0.22.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchtext in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (0.18.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rf\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_ops.py:1392\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1387\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ctypes\\__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
     ]
    }
   ],
   "source": [
    "# If running in Colab or Jupyter, uncomment the line below to install dependencies:\n",
    "!pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer and dataset\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = list(IMDB(split='train'))  # Convert iterator to list for multiple passes\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Text encoding\n",
    "def encode(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label == 'pos' else 0)\n",
    "        texts.append(torch.tensor(encode(text), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels, dtype=torch.float32), texts\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<unk>\"])\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0)).squeeze(1)  # Output shape: (batch,)\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    model = GRUModel(len(vocab), embed_dim=64, hidden_dim=128, output_dim=1).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Reload train data for DataLoader\n",
    "    train_data = list(IMDB(split='train'))\n",
    "    dataloader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for labels, texts in dataloader:\n",
    "            labels, texts = labels.to(device), texts.to(device)\n",
    "            preds = model(texts)\n",
    "            loss = criterion(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.sigmoid(preds) > 0.5\n",
    "            correct += (predictions.float() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}, Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "# Run training\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a24f0d-7c70-4036-b6b9-7f938b7c48a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]  \u001b[38;5;66;03m# (label, text)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Build vocabulary from training data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_data \u001b[38;5;241m=\u001b[39m TextDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your CSV path\u001b[39;00m\n\u001b[0;32m     29\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, text \u001b[38;5;129;01min\u001b[39;00m train_data:\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;66;03m# CSV format: label (0/1), text\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# Custom tokenizer and dataset implementation\n",
    "def basic_english_tokenizer(text):\n",
    "    return text.lower().replace('.', ' ').replace(',', ' ').split()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.samples = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                # CSV format: label (0/1), text\n",
    "                self.samples.append((int(row[0]), row[1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # (label, text)\n",
    "\n",
    "# Build vocabulary from training data\n",
    "train_data = TextDataset('train.csv')  # Replace with your CSV path\n",
    "\n",
    "all_tokens = []\n",
    "for label, text in train_data:\n",
    "    all_tokens.extend(basic_english_tokenizer(text))\n",
    "\n",
    "# Create vocabulary with <unk> token\n",
    "vocab = {'<unk>': 0}\n",
    "for token in all_tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "def text_to_indices(text):\n",
    "    return [vocab.get(token, 0) for token in basic_english_tokenizer(text)]\n",
    "\n",
    "# Collate function with padding\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(label)\n",
    "        texts.append(torch.tensor(text_to_indices(text), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels, dtype=torch.float32), texts\n",
    "\n",
    "# GRU Model\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0)).squeeze(1)\n",
    "\n",
    "# Training setup\n",
    "def train():\n",
    "    model = GRUClassifier(len(vocab))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    dataloader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for labels, texts in dataloader:\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f} | Acc: {correct/total:.4f}\")\n",
    "\n",
    "# Start training\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14708ccd-74fb-4931-ab91-dd60a42d1ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# Install needed packages\n",
    "# !pip install torch torchtext datasets --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Load IMDB dataset from HuggingFace\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# Tokenizer and Vocabulary\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for example in data_iter:\n",
    "        yield tokenizer(example[\"text\"])\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Encode text\n",
    "def encode(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Collate function\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for example in batch:\n",
    "        labels.append(1 if example[\"label\"] == 1 else 0)\n",
    "        texts.append(torch.tensor(encode(example[\"text\"]), dtype=torch.int64))\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels), texts\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    model = GRUModel(len(vocab), 64, 128, 1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    dataloader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for labels, texts in dataloader:\n",
    "            preds = model(texts).squeeze()\n",
    "            loss = criterion(preds, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Start training\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba22b0a-9622-4210-b1e3-3a7eadce9c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
