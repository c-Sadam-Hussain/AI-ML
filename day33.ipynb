{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb87d8c-f176-4d3b-bfbe-4b4939a66c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings\n",
    "in word embedding dimension is in between 50 and 300 \n",
    "if it is greater we reduce it in 2D or 3D (we use PCA library)\n",
    "word to vector in 2013\n",
    "This enables machines to understand semantic relationships between words.\n",
    "    \n",
    "1.continuous BOW (big of word)\n",
    "    Goal: Predict a target word based on its context (surrounding words).\n",
    "Sentence: \"The cat sits on the mat.\"\n",
    "Context: [\"The\", \"sits\", \"on\", \"the\"] → Target: \"cat\"\n",
    "better for smaller data set\n",
    "fast\n",
    "#architecture\n",
    "input layer (context word) (one hot vector)\n",
    "hidden layer (progection layer) \n",
    "output layer (probability vector through softmax function)\n",
    "The model takes the surrounding words as input and tries to guess the center word.\n",
    "\n",
    "2.Continuous Skip-gram\n",
    "    Goal: Predict the context words given a target word.\n",
    "    Sentence: \"The cat sits on the mat.\"\n",
    "Target: \"cat\" → Context: [\"The\", \"sits\", \"on\", \"the\"]\n",
    "The model takes a single word as input and tries to predict words around it\n",
    "preffered for larger dataset\n",
    "slow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6be04c1-6f07-42aa-8217-7adb8cad90ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.1472\n",
      "Epoch 10, Loss: 2.0958\n",
      "Epoch 20, Loss: 2.0473\n",
      "Epoch 30, Loss: 2.0012\n",
      "Epoch 40, Loss: 1.9571\n",
      "Epoch 50, Loss: 1.9145\n",
      "Epoch 60, Loss: 1.8732\n",
      "Epoch 70, Loss: 1.8329\n",
      "Epoch 80, Loss: 1.7933\n",
      "Epoch 90, Loss: 1.7544\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "text = \"the quick brown fox jumps over the lazy dog\".split()\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = set(text)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Generate CBOW training pairs\n",
    "def generate_cbow_pairs(text, window_size=2):\n",
    "    pairs = []\n",
    "    for i in range(window_size, len(text) - window_size):\n",
    "        context = [text[i - j - 1] for j in range(window_size)] + \\\n",
    "                  [text[i + j + 1] for j in range(window_size)]\n",
    "        target = text[i]\n",
    "        pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = generate_cbow_pairs(text)\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot(index):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "# Prepare input and output tensors\n",
    "X = []\n",
    "Y = []\n",
    "for context, target in cbow_pairs:\n",
    "    context_vec = np.sum([one_hot(word2idx[w]) for w in context], axis=0)\n",
    "    X.append(context_vec)\n",
    "    Y.append(word2idx[target])\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "Y = torch.LongTensor(Y)\n",
    "\n",
    "# CBOW Model Definition\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Linear(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = CBOW(vocab_size, 10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = loss_fn(output, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4064a07c-aa23-438d-bfda-a779d5f15011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.2462\n",
      "Epoch 10, Loss: 2.1799\n",
      "Epoch 20, Loss: 2.1206\n",
      "Epoch 30, Loss: 2.0664\n",
      "Epoch 40, Loss: 2.0163\n",
      "Epoch 50, Loss: 1.9694\n",
      "Epoch 60, Loss: 1.9250\n",
      "Epoch 70, Loss: 1.8826\n",
      "Epoch 80, Loss: 1.8420\n",
      "Epoch 90, Loss: 1.8028\n",
      "\n",
      "Predicted center word for context ['the', 'quick', 'fox', 'jumps']: 'brown'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "text = \"the quick brown fox jumps over the lazy dog\".split()\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = set(text)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Generate CBOW training pairs\n",
    "def generate_cbow_pairs(text, window_size=2):\n",
    "    pairs = []\n",
    "    for i in range(window_size, len(text) - window_size):\n",
    "        context = [text[i - j - 1] for j in range(window_size)] + \\\n",
    "                  [text[i + j + 1] for j in range(window_size)]\n",
    "        target = text[i]\n",
    "        pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = generate_cbow_pairs(text)\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot(index):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "# Prepare input and output tensors\n",
    "X = []\n",
    "Y = []\n",
    "for context, target in cbow_pairs:\n",
    "    context_vec = np.sum([one_hot(word2idx[w]) for w in context], axis=0)\n",
    "    X.append(context_vec)\n",
    "    Y.append(word2idx[target])\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "Y = torch.LongTensor(Y)\n",
    "\n",
    "# CBOW Model Definition\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Linear(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = CBOW(vocab_size, 10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = loss_fn(output, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Prediction after training\n",
    "test_context = [\"the\", \"quick\", \"fox\", \"jumps\"]  # context for predicting \"brown\"\n",
    "context_vec = np.sum([one_hot(word2idx[w]) for w in test_context], axis=0)\n",
    "context_tensor = torch.Tensor([context_vec])  # batch dimension\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(context_tensor)\n",
    "    predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    predicted_word = idx2word[predicted_idx]\n",
    "\n",
    "print(f\"\\nPredicted center word for context {test_context}: '{predicted_word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb72c1-2d8a-45a9-b5f4-472679963ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignment\n",
    "\n",
    "Build a simple CBOW model to learn word embeddings using a small corpus. You'll:\n",
    "\n",
    "Create a vocabulary\n",
    "\n",
    "Generate context-target pairs\n",
    "\n",
    "One-hot encode words\n",
    "\n",
    "Train a shallow neural network with one hidden layer to predict the center word from surrounding context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39cb010-ad7c-4de1-ae8e-16946b98b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6582\n",
      "Epoch 20, Loss: 1.7392\n",
      "Epoch 40, Loss: 0.7359\n",
      "Epoch 60, Loss: 0.2351\n",
      "Epoch 80, Loss: 0.0838\n",
      "Epoch 100, Loss: 0.0404\n",
      "Epoch 120, Loss: 0.0248\n",
      "Epoch 140, Loss: 0.0174\n",
      "Epoch 160, Loss: 0.0131\n",
      "Epoch 180, Loss: 0.0103\n",
      "\n",
      "Learned word embeddings:\n",
      "    a: [ 0.97888726  0.88240165  0.973537    0.20303002 -0.05467448  0.30904806\n",
      "  0.29014635 -0.46173197  0.13534695 -0.7366295 ]\n",
      "  and: [-0.01963021  0.4019308   0.7408129   0.9595285  -0.13770202  0.46147734\n",
      " -0.07047568  0.23587021 -0.81130284  0.07828525]\n",
      "because: [ 0.9554598   0.5189271  -0.8518682   0.7884116  -0.25491792 -0.32067814\n",
      " -1.1183769  -0.7293538   0.5239446  -0.5143575 ]\n",
      "football: [ 0.01161816 -0.17910893 -0.93798995  0.91277957 -0.05154774  0.6004798\n",
      "  0.80328107 -0.01622613  0.8789514  -0.70501494]\n",
      "great: [ 0.8490904   0.8564599   0.44155273 -1.067832    0.17103451 -0.8941354\n",
      "  1.2274886   0.8291184   0.6733979   0.10236617]\n",
      "happy: [-0.2348434  -0.17415917 -0.9824613  -0.42830747  0.07543609 -0.57974344\n",
      " -0.28457463 -0.09056619  0.06750601  1.190278  ]\n",
      "    i: [-1.1527332  -0.32238722 -0.10528845  1.0135907   0.1207424  -1.1774641\n",
      " -0.08956897 -0.13421343  1.0078216   0.04888993]\n",
      "   is: [-0.3356227   0.9299799  -0.08196035  1.0977749  -0.03478519  1.1629771\n",
      "  0.1563086   0.75876206  0.4803495  -0.3272372 ]\n",
      "   it: [-0.81323886 -0.7420594  -1.1167394   0.23952755 -0.26718652 -0.87057906\n",
      " -0.4344571   0.7173054  -0.55835235  0.6840615 ]\n",
      " love: [ 0.5281487  -0.46415272  0.03853926  0.843391    0.12140723  0.5912963\n",
      " -0.758638   -0.01832     0.3416916  -0.32527083]\n",
      "makes: [-0.4519877  -0.56392866  0.89446926  0.963835   -0.18786919 -0.795451\n",
      " -0.8893291   0.25384933 -0.02266715  0.77224845]\n",
      "   me: [-0.3329002   0.04317588  0.3595536  -0.13736285 -0.14265895 -0.3584782\n",
      " -0.35453153 -1.069232    0.1151869   0.9340078 ]\n",
      "playing: [ 0.31242186 -1.0182196   0.8422053  -0.27934715  0.11754788  1.4994094\n",
      "  0.36550817  0.13115941 -0.4287837   0.8267016 ]\n",
      "sport: [-0.79949427  0.45876753  0.1298628   0.49705985 -0.25397536  0.89716995\n",
      "  0.686566    1.140284   -0.11746343  0.21152012]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 1. Sample corpus\n",
    "corpus =\"I love playing football because football is a great sport and playing it makes me happy\".lower().split()\n",
    "\n",
    "# 2. Create vocabulary\n",
    "vocab = sorted(set(corpus))\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 3. Generate context-target pairs (window size = 2)\n",
    "def generate_cbow_data(text, window_size=2):\n",
    "    pairs = []\n",
    "    for i in range(window_size, len(text) - window_size):\n",
    "        context = [text[i - j - 1] for j in range(window_size)] + \\\n",
    "                  [text[i + j + 1] for j in range(window_size)]\n",
    "        target = text[i]\n",
    "        pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = generate_cbow_data(corpus)\n",
    "\n",
    "# 4. One-hot encode context words and prepare input/output tensors\n",
    "def one_hot_vector(index, vocab_size):\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for context, target in cbow_pairs:\n",
    "    context_vec = np.sum([one_hot_vector(word2idx[word], vocab_size) for word in context], axis=0)\n",
    "    X.append(context_vec)\n",
    "    Y.append(word2idx[target])\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "Y = torch.LongTensor(Y)\n",
    "\n",
    "# 5. Define CBOW model (shallow NN with one hidden layer)\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.hidden = nn.Linear(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.relu(x)  # Activation in hidden layer\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# 6. Initialize model, loss, optimizer\n",
    "embedding_dim = 10\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 7. Training loop\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = loss_fn(output, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 8. Visualize embeddings (optional)\n",
    "embeddings = model.hidden.weight.data\n",
    "print(\"\\nLearned word embeddings:\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word:>5}: {embeddings[:, idx].detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97265020-597a-4a4f-bf55-dd2a86b3fca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
